{
    "warc" : {
        "title" : "Default indexer config."
        # Indexing configuration:
        "index" : {
            # What to extract:
            "extract" : {
                # Maximum payload size allowed to be kept wholly in RAM:
                "inMemoryThreshold" : 10M,
                # Maximum payload size that will be serialised out to disk instead of held in RAM:
                "onDiskThreshold" : 100M,
                
                # Content to extract
                "content" : {
                    # Should we index the content body text?
                    "text" : true,
                    
                    # Should we store the content body text?
                    "text_stored" : true,
                    
                    # Extract list of elements used in HTML:
                    "elements_used" : true,
                    
                    # Extract potential PDF problems using Apache PDFBox Preflight:
                    "extractApachePreflightErrors" : true,
                    
                    # Extract image features:
                    "images" : {
                        "enabled" : true,
                        "maxSizeInBytes" : 1M,
                    	"detectFaces" : true,
                    	"dominantColours" : true,
                        # The random sampling rate:
                        # (where '1' means 'extract from all images', 
                        # and '100' would mean 'extract from 1 out of every 100 images')
                        "analysisSamplingRate": 10
                    }
                    
                    # Extract the first bytes of the file (for shingling):
                    "first_bytes" : {
                        # Enabled?
                        "enabled" : true,
                        # Number of bytes to extract (>=4 to allow content_ffb to work):
                        "num_bytes" : 32
                    }
                },
                
                # Which linked entities to extract:
                "linked" : {
                    "resources" : true,
                    "hosts" : true,
                    "domains" : true,
                    "images" : true
                },
                
                # Restrict record types:
                "record_type_include" : [
                    response, revisit
                ],
                
                # Restrict response codes:
                # works by matches starting with the characters, so "2" will match 2xx:
                "response_include" : [
                    "2"
                ],
    
                # Restrict protocols:
                "protocol_include" : [
                    http,
                    https
                ],
   
                # URLs to skip, e.g. ['robots.txt'] ???
                "url_exclude" : [],
            },
            
            # Parameters relating to format identification:   
            "id" : {
                # Allow tools to infer format from the resource URI (file extension):
                "useResourceURI" : true
    
                # DROID-specific config:
                "droid" : {
                    "enabled" : true,
                    "useBinarySignaturesOnly" : false
                },
            },
            
            # Parameters to control Apache Tika behaviour
            "tika" : {
                # Maximum length of text to extract:
                "max_text_length" : 512K,
                # Should we use the 'boilerpipe' text extractor?:
                "use_boilerpipe": false,
                # The parse timeout (for when Tika gets stuck):
                "parse_timeout" : 300000,
                # Formats to avoid processing
                "exclude_mime" : [
                    "x-tar",
                    "x-gzip",
                    "bz",
                    "lz",
                    "compress",
                    "zip",
                    "javascript",
                    "css",
                    "octet-stream"
                ],
                # Should we extract all the available metadata:
                "extract_all_metadata": false
            },
            
            # Parameters to control the exclusion of results from the indexing process:
            "exclusions" : {
                # Exclusion enabled?
                "enabled" : false,
                # Default check interval before reloading the exclusions file, in seconds:
                "check_interval" : 600,
                # Exclusion URI/SURT prefix file:
                "file" : "/path/to/exclude.txt"
            }
        },
        
        # Solr configuration:
        "solr" : {
            # Use the hash+url as the ID for the documents
            "use_hash_url_id": true
            # Check SOLR for duplicates during indexing:
            "check_solr_for_duplicates": false
            # Server configuration:
            "server" : "http://localhost:8080/solr/discovery",
            # Solr document batch size for submissions:
            "batch_size" : 50,
            # Is this a dummy-run? (i.e. should we NOT post to SOLR?)
            "dummy_run" : false,
            # Disable explicit commit
            "disablecommit" : false,
        },
        
        # HTTP Proxy to use when talking to Solr (if any):
        "http_proxy" : {
            #"host" : explorer.bl.uk,
            #"port" : 3127
        }
    }
}
    